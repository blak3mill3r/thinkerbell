todo
  biases
    allocate temp memory for them very much like weights, 3 for training-vertices
    allocate "permanent" memory for them much like weights, 1 space for non-training-vertices
    place in the host class Neurons for them
    kernels to adjust them
    put them in the algorithm in DBNScheduler

  organization of example data
    needs to be randomized order on the host
    host buffer size has to be at least device_buffer_size * 3
    
    


little things
name changes
  vertex_ptr -> neurons_ptr
  DeepBeliefNetwork* -> DBN*
move all the DBN* classes into a namespace deep_belief_network




           |---|   |
           |   |   |
           | A |---|
           |   |   |
           |---| B |
           |   |   |
           | C |---|
           |   |   |
           |---| A |
           |   |   |
           | B |---|
           |   |   |
   t0   -->|---| C |
           |   |   |
           | A |---|<--    t1
           |   |   |
   t2   -->|---| B |
           |   |   |
           | C |---|
           |   |   |


at t0
synch with A-exec-finished in other stream
up activate through graph
synch with C-exec-finished in other stream
positive weight adjust step, weights-C = weights-B + delta
2 ags steps
negative weight adjust step, weights-C = weights-C - delta
record A-exec-finished

at t1
synch with B-exec-finished in other stream
up activate through graph
synch with A-exec-finished in other stream
positive weight adjust step, weights-A = weights-C + delta
2 ags steps
negative weight adjust step, weights-A = weights-A - delta
record B-exec-finished

